{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyOZQ1danu19FCtGdG3pFTRh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 00. Pytorch Fundamentals\n","\n","Resource notebook: https://www.learnpytorch.io/00_pytorch_fundamentals/\n","\n","If you have a question: https://github.com/mrdbourke/pytorch-deep-learning/discussions"],"metadata":{"id":"hv1Is-yNURR8"}},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","print(torch.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3911-HPTN6j","executionInfo":{"status":"ok","timestamp":1715778936818,"user_tz":-120,"elapsed":5666,"user":{"displayName":"Michel Calvaresi","userId":"16873638640678374357"}},"outputId":"6c659179-6393-4770-879c-c0a44a4c72d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.1+cu121\n"]}]},{"cell_type":"markdown","source":["## Running tensors and PyTorch objects on the GPUs (and making faster computations)\n","\n","GPU = faster computation on numbers, thanks to CUDA + NVIDIA hardware + PyTorch working behind the scenes to make everything hunky dory (good)."],"metadata":{"id":"n_Hmouj3zWhe"}},{"cell_type":"markdown","source":["### 1. Getting a GPU\n","\n","1. Easiest - Use Google Colab for a free GPU (options to upgrade as well)\n","2. Use your own GPU - takes a little bit of setup and requires the investiment of purchasing a GPU, there's a lot of options... see posts online\n","3. Use cloud computing - GCP AWS, Azure, these services allow you to rent computers on the cloud and access them\n","\n","For 2, 3 Pytorch + GPU drivers (CUDA) takes a little bit of setting up to do this, refer to Pytorch setup documentation"],"metadata":{"id":"zQAgq-ekzhU6"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q2Y8xrBCzDDD","executionInfo":{"status":"ok","timestamp":1715853307098,"user_tz":-120,"elapsed":302,"user":{"displayName":"Michel Calvaresi","userId":"16873638640678374357"}},"outputId":"b55c079d-cf02-4f33-cc08-6478450e6f54"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu May 16 09:55:06 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   59C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["### 2. Check for GPU access with Pytorch"],"metadata":{"id":"ChxhdZ_a1MO9"}},{"cell_type":"code","source":["# Check for GPU access with PyTorch\n","import torch\n","torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uuLfHSXB06lk","executionInfo":{"status":"ok","timestamp":1715853537870,"user_tz":-120,"elapsed":4386,"user":{"displayName":"Michel Calvaresi","userId":"16873638640678374357"}},"outputId":"1a6b8c8c-e636-418a-b23b-448ff5d257d7"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["For Pytorch since it's capable of running compute on the GPU or CPU, it's best practice to setup device agnostic code:\n","https://pytorch.org/docs/stable/notes/cuda.html#best-practices\n","E.g. run on GPU if available, else default to GPU"],"metadata":{"id":"lmT3n8EU2Xlv"}},{"cell_type":"code","source":["# Setup device agnostic code\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"GYVruJYy1lao","executionInfo":{"status":"ok","timestamp":1715853595560,"user_tz":-120,"elapsed":348,"user":{"displayName":"Michel Calvaresi","userId":"16873638640678374357"}},"outputId":"87d72000-2f9c-4ffc-ae9e-3b5bfdda528f"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Count number of devices\n","torch.cuda.device_count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PUD6LxLO2BCT","executionInfo":{"status":"ok","timestamp":1715853624808,"user_tz":-120,"elapsed":238,"user":{"displayName":"Michel Calvaresi","userId":"16873638640678374357"}},"outputId":"569a3e9d-6d0e-4b37-e2bf-899ecbc23808"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["### 3. Putting tensors (and models) on the GPU\n","\n","The reason we want out tensors/models on the GPU is because using a GPU results in faster computations."],"metadata":{"id":"GD0hQPoS3Cut"}},{"cell_type":"code","source":["# Create a tensor (default on the CPU)\n","tensor = torch.tensor([1, 2, 3])\n","\n","# Tensor not on GPU\n","print(tensor, tensor.device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CF9MNrzX2IO2","executionInfo":{"status":"ok","timestamp":1715853986432,"user_tz":-120,"elapsed":215,"user":{"displayName":"Michel Calvaresi","userId":"16873638640678374357"}},"outputId":"33c8904d-d6ac-478a-ef44-04b4eac35d27"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3]) cpu\n"]}]},{"cell_type":"code","source":["# Move tensor to GPU (if available)\n","tensor_on_gpu = tensor.to(device)\n","tensor_on_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ovSu9vcR3ghB","executionInfo":{"status":"ok","timestamp":1715854043635,"user_tz":-120,"elapsed":543,"user":{"displayName":"Michel Calvaresi","userId":"16873638640678374357"}},"outputId":"69e58079-d3c1-4ce1-80d2-c745b1cfab56"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3], device='cuda:0')"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["### 4. Moving tensors back to the CPU"],"metadata":{"id":"WGz5mFiK4CJ6"}},{"cell_type":"code","source":["# If tensor is on GPU, can't transform it to Numpy\n","tensor_on_gpu.numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213},"id":"m8F06mPH3uaS","executionInfo":{"status":"error","timestamp":1715854172188,"user_tz":-120,"elapsed":214,"user":{"displayName":"Michel Calvaresi","userId":"16873638640678374357"}},"outputId":"b681a4f0-b990-4206-bcb2-7e83928f4854"},"execution_count":7,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-2cc50256f9ac>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If tensor is on GPU, can't transform it to Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtensor_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."]}]},{"cell_type":"code","source":["# To fix the GPU tensor with Numpy issue, we can first set it to the CPU\n","tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n","tensor_back_on_cpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43NvJfem4N3O","executionInfo":{"status":"ok","timestamp":1715854237720,"user_tz":-120,"elapsed":262,"user":{"displayName":"Michel Calvaresi","userId":"16873638640678374357"}},"outputId":"625e6f92-85e3-46cd-8464-824231b20c69"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 2, 3])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["tensor_on_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ulEHF3d4d3O","executionInfo":{"status":"ok","timestamp":1715854258651,"user_tz":-120,"elapsed":241,"user":{"displayName":"Michel Calvaresi","userId":"16873638640678374357"}},"outputId":"f14ddead-ac75-4ea2-f42d-5a011a436854"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3], device='cuda:0')"]},"metadata":{},"execution_count":9}]}]}